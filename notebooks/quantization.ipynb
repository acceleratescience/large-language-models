{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large portions of this code are taken from the following sources:\n",
    "- [Hugging Face PEFT LoRA tutorials](https://huggingface.co/docs/peft/en/developer_guides/lora)\n",
    "- [Hugging Face PEFT quantization tutorials](https://huggingface.co/docs/peft/en/developer_guides/quantization)\n",
    "\n",
    "Please check out the original sources for more information, and other amazing tutorials. I strongly recommend the[Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt) for a good overview of the Hugging Face `transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rkd/Desktop/Teaching/large-language-models/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure BitsAndBytes to quantize a model. Here is an explaination of the parameters:\n",
    "\n",
    "`load_in_4bit` - Load the model in 4-bit precision.\n",
    "\n",
    "`bnb_4bit_quant_type` - What type of quantization do you want to use. Here we use `\"nf4\"`, which is a type of quantile quantization. The weights are normalized to the range $[-1, 1]$ and binned into one of 16 bins. For more details see the [QLoRA paper](https://arxiv.org/abs/2305.14314).\n",
    "\n",
    "`bnb_4bit_use_double_quant` - After the weights are quantized to 4-bits using a technique like NF4, quantization constants (absolute max values for each quantization block, usually stored in FP32) must still be stored to allow dequantizing the weights during computation. For large models with many quantization blocks, storing these constants adds non-trivial memory overhead. Double quantization addresses this by performing a second round of quantization, this time on the 32-bit quantization constants themselves.\n",
    "\n",
    "`bnb_4bit_compute_dtype` - Here we use the `bfloat` type. This is similar to the standard 16-bit half-precision, but the ratio of bits assigned to the exponent and mantissa is different (8 and 7 bits, respectively vs 5 and 10 bits with both having 1 bit for the sign). This allows for a larger range of values to be represented, which is useful for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a file called `.env` to store the environment variables. This is a good practice to avoid hardcoding sensitive information in the code. However if you are running this code in Colab, you can just do:\n",
    "```python\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = 'hf_...'\n",
    "login(token=HF_TOKEN)\n",
    "```\n",
    "\n",
    "and NOT run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/rkd/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model. Note that this will not work if you're running on a CPU. You need to be connected to a GPU either on Colab or through a local GPU. If running in Colab, and you get an error, try restarting the runtime and running the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/CodeLlama-7b-Instruct-hf\", quantization_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(**inputs,\n",
    "                            max_length=512,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.1,\n",
    "                            top_k=10, top_p=0.95,\n",
    "                            num_return_sequences=1,eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat(\"What is a good machine learning library for Python?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to prepare the model for PEFT..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can now be trained in the usual way, which is trivial and left as an exercise to the reader..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
