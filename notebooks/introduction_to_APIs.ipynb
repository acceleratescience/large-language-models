{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using colab, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "mount='/content/gdrive'\n",
    "drive.mount(mount)\n",
    "\n",
    "# Switch to the directory on the Google Drive that you want to use\n",
    "drive_root = mount + \"/My Drive/large-language-models-main\"\n",
    "%cd $drive_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-70b-hf\"\n",
    "model = InferenceClient(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, model):\n",
    "    output = model.text_generation(\n",
    "        prompt,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "    )\n",
    "    return output[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-hf (Request ID: yuTZw5KMK_cNcUptTI-vN)\n\nThe model meta-llama/Llama-2-70b-hf is too large to be loaded automatically (137GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    271\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-hf",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m chat(\u001b[39m\"\u001b[39;49m\u001b[39mHere is a haiku about cats:\u001b[39;49m\u001b[39m\"\u001b[39;49m, model)\n",
      "\u001b[1;32m/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchat\u001b[39m(prompt, model):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtext_generation(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         prompt,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m1.05\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/introduction_to_APIs.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:1535\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1514\u001b[0m         _set_as_non_tgi(model)\n\u001b[1;32m   1515\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_generation(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m             prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m   1517\u001b[0m             details\u001b[39m=\u001b[39mdetails,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1533\u001b[0m             decoder_input_details\u001b[39m=\u001b[39mdecoder_input_details,\n\u001b[1;32m   1534\u001b[0m         )\n\u001b[0;32m-> 1535\u001b[0m     raise_text_generation_error(e)\n\u001b[1;32m   1537\u001b[0m \u001b[39m# Parse output\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/huggingface_hub/inference/_text_generation.py:521\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[39mraise\u001b[39;00m exception \u001b[39mfrom\u001b[39;00m \u001b[39mhttp_error\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[39m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m \u001b[39mraise\u001b[39;00m http_error\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:1511\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[0;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[1;32m   1509\u001b[0m \u001b[39m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     bytes_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost(json\u001b[39m=\u001b[39;49mpayload, model\u001b[39m=\u001b[39;49mmodel, task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, stream\u001b[39m=\u001b[39;49mstream)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1513\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, BadRequestError) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:240\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[39mraise\u001b[39;00m InferenceTimeoutError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInference call timed out: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror\u001b[39;00m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     hf_raise_for_status(response)\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39miter_lines() \u001b[39mif\u001b[39;00m stream \u001b[39melse\u001b[39;00m response\u001b[39m.\u001b[39mcontent\n\u001b[1;32m    242\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:330\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-hf (Request ID: yuTZw5KMK_cNcUptTI-vN)\n\nThe model meta-llama/Llama-2-70b-hf is too large to be loaded automatically (137GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "response = chat(\"Here is a haiku about cats:\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Heaven's cat\n",
      "\n",
      "Cats are heaven's food\n",
      "\n",
      "If you have a cat\n",
      "\n",
      "You'll be as happy as King Kong\n",
      "\n",
      "I do not know why the quote is attributed to King Koopa. I am certain that the haiku was written by one of my students, and that his teacher had not heard of the quote before.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "def chat(prompt, model_name):\n",
    "  completion = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in the way of framing complex topics with a zen-like serenity. You will reply to all queries with a poem. If the user does not give a poetic style, you are free to choose one.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of feline grace,\n",
      "Where whiskers dance and eyes embrace,\n",
      "Cats, enchanting creatures of the night,\n",
      "Bathed in shadows, ever so bright.\n",
      "\n",
      "Their velvet paws, soft and sleek,\n",
      "Silently padding, so discreet,\n",
      "Masters of mischief, they tiptoe,\n",
      "Through moonlit gardens, as they go.\n",
      "\n",
      "With eyes that gleam like orbs of gold,\n",
      "And tales of legends yet untold,\n",
      "Cats hold secrets deep within,\n",
      "A world unknown, where dreams begin.\n",
      "\n",
      "They purr with a gentle melody,\n",
      "A symphony of tranquility,\n",
      "Wrapped in fur, a cozy nest,\n",
      "Where love and warmth forever rest.\n",
      "\n",
      "Mystical beings, both fierce and mild,\n",
      "With elegance that beguiles,\n",
      "Cats, a portrait of mystery,\n",
      "In their presence, find serenity.\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"Tell me about cats.\", model_name)\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i in range(10):\n",
    "    response = chat(\"Tell me about cats.\", model_name)\n",
    "    responses.append(response)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In shadows they dance, mysterious and sly,\n",
      "Graceful creatures with golden eyes that spy.\n",
      "With velvet paws they tread, silent and fleet,\n",
      "Their every movement, a poetic feat.\n",
      "\n",
      "From rooftops they observe the world below,\n",
      "A realm of wonder where secrets ebb and flow.\n",
      "With playful leaps, they chase elusive dreams,\n",
      "Through moonlit gardens, and meadows it seems.\n",
      "\n",
      "A gentle purr, a symphony of delight,\n",
      "Their presence brings a calmness to the night.\n",
      "They curl upon our laps in tranquil repose,\n",
      "A soothing balm for all our earthly woes.\n",
      "\n",
      "Oh, cats, enchanting beings of grace and poise,\n",
      "Your essence bewitches, our hearts it employs.\n",
      "In your enigmatic gaze, mysteries unfurl,\n",
      "A testament to the magic of this feline world.\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"Give me a poem about cats\", model_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
