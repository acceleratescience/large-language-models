{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning GPT-2 with custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either use `AutoModelWithLMHead` and specify that we want `\"gpt2\"`, or we can use `GPT2LMHeadModel`. The first option is preferable because it means we can use any model with an LM head. We are using GPT-2, the smallest version. There are also `gpt2-medium`, `gpt2-large`, and `gpt2-xl`.\n",
    "\n",
    "**What is an LM Head?**\n",
    "\n",
    "It is the Language Model head. It is the fully connected neural network layer that maps the high-dimensional output of the transformer to the size of the vocabulary used in the model. This part of the network produces the probability distribution over the tokens in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(\"cuda\") # meta-llama/Llama-2-13b-hf\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is quite small, but still very effective. We write a function that will prompt the model for us. `model.generate` takes a few arguments. Here are the important ones:\n",
    "\n",
    "`max_length`: How many tokens do you want the model to output? If you set this too long, you might get repetition.\n",
    "\n",
    "`temperature`: How random do you want the output to be. 0 is not very random, and 1 is highly random.\n",
    "\n",
    "`no_repeat_ngram_size`: All ngrams of this size can only occur this many times. An ngram is a series of adjacent tokens. So in other words if this is 2, then all ngrams of size 2 can only occur once.\n",
    "\n",
    "`do_sample`: Whether or not to sample. If False, you'll get the same output every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have any regrets?\n",
      "\n",
      "No, I'm not regretful at all. It's just that I didn't want to do it. I just wanted to go out there and do what I love doing. That's why I did it, because I knew I was going to be a good person and I had a lot of respect for the people that were around me. But I also knew that it would be hard for me to make it to the end of the season, so I wasn't sure if I'd be able to come back and play in the playoffs or not. So I decided to take it one step at a time, and that's how I ended up doing it.\"\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(input_ids,\n",
    "                        max_length=256,\n",
    "                        # temperature=0.7,\n",
    "                        num_beams=5,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        early_stopping=True,\n",
    "                        # do_sample=True,\n",
    "                        # pad_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "output = generate(\"Do you have any regrets?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above you'll probably get a coherent, but meaningless response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further pretraining on a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use transcripts of press events from former President Clinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_text('../sample_data/cleaned_test_text_1.txt', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some messing around with the `datasets` library to get this to work. We tokenize the text, cut the text into chunks, and put it into a format the Hugging Face trainer can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    \n",
    "{\"input_ids\": outputs.input_ids}\n",
    "\n",
    "tokenized_dataset = Dataset.from_dict({\"input_ids\": outputs.input_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 256])\n",
      "attention_mask shape: torch.Size([5, 256])\n",
      "labels shape: torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = 'tokenizer.eos_token'\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "out = data_collator([tokenized_dataset[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have any regrets? The President. Well, first of all, I didn't do anything wrong. I was just trying to do my job and try to help the American people. And I think that's the most important thing that I could have done. Secondly, we had a lot of good people in the White House, and we did a good job there. We had some bad people there, too. So I don't regret what I did, but I do regret the fact that we were able to get people to come to our office and say, \"Mr. President, you know, this is what we're doing here,\" and then we got a chance to talk to them and get a better sense of what was going on. Mr. Blitzer. Do you regret that, personally, that you were the only person in this room who was not inaudible to the people who were there at the dinner table when the President was talking to you? President Bush. No, not at all. That's not what happened. It happened in '96, '98. But I'm very sorry about it, because I thought it was a terrible mistake to say anything that would have been interpreted as a threat to me or to anybody else. You know\n"
     ]
    }
   ],
   "source": [
    "output = generate(\"Do you have any regrets?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model(\"../models/basic_model/\")\n",
    "\n",
    "# clear gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is definitely learning the data. Now let's see if we can make it learn a general QA format. To do this, we recognize some of the reoccuring features of the data set, such as responses being given by `The President.` We look for all instances of `The President.` and replace with `RESPONSE: `. We also search for names of the interviewers and replace with `QUESTION: `. We also include the end of sentence token at the end of the responses, in the hopes that it will also learn these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_text('../sample_data/cleaned_test_text_1_QA.txt', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 256])\n",
      "attention_mask shape: torch.Size([5, 256])\n",
      "labels shape: torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = 256\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "  model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "{\"input_ids\": outputs.input_ids}\n",
    "\n",
    "tokenized_dataset = Dataset.from_dict({\"input_ids\": outputs.input_ids})\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "out = data_collator([tokenized_dataset[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really the process is just identical to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1400 04:14, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.630600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.427100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.262700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.020500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1400, training_loss=2.5990921129499163, metrics={'train_runtime': 254.8962, 'train_samples_per_second': 43.783, 'train_steps_per_second': 5.492, 'total_flos': 1458009538560000.0, 'train_loss': 2.5990921129499163, 'epoch': 20.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We slightly change our prompt and display templates, just to make it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(input_ids,\n",
    "                        max_length=len(input_ids[0])+128,\n",
    "                        temperature=0.1,\n",
    "                        num_beams=5,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What is your biggest regret?\n",
      "\n",
      "RESPONSE: Well, first of all, I'm not sure that I could have made it if I had known what I wanted to do and what it would cost me. But I think it's very important to me that the American people have a clear idea of what they're going to spend their money on, and they should be able to make their own judgment about what's best for them and their families.\n"
     ]
    }
   ],
   "source": [
    "question = \"QUESTION: What is your biggest regret?\"\n",
    "prompt = f\"{question} RESPONSE:\"\n",
    "output = generate(prompt)[len(prompt):]\n",
    "print(f\"{question}\\n\\nRESPONSE:{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QUESTION: What did you talk to President Kim about today? RESPONSE: Well, first of all, I talked to him about the North\\'s missile program and the missile defense program. And he said, \"I don\\'t think it\\'s appropriate for me to discuss that right now, because I\\'m not going to comment on North Korea\\'s program, but I think the program is in the best interest of the United States and our partners in South Korea and around the world for the long term security of our two countries.\" And that\\'s what I did. I didn\\'t say anything that was demeaning or offensive. What I said was that I thought was important to the American people, and it was the right'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"QUESTION: What did you talk to President Kim about today? RESPONSE:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model(\"../models/QA_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is working somewhat. Note that GPT-2 is a very small model, and the dataset is also small. In general, the results will be quite poor. You can try rerunning this on `gpt2-medium` or `gpt2-large` if you have the compute and memory. On my machine, this whole notebook will consume about 12GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreival Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(text_path, chunk_size, chunk_overlap):\n",
    "    # load text\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split text\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=' ',\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "    db = FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "    return db\n",
    "\n",
    "text_path = '../sample_data/cleaned_test_text_1.txt'\n",
    "db = create_database(text_path, chunk_size=256, chunk_overlap=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that, but I don't. But the thing I regret most, except for doing the wrong thing, is misleading the American people about it. I do not regret the fact that I fought the Independent Counsel. And what they did was, in that case and generally, was completely\n",
      "\n",
      "of the world. I think we've basically been a force for peace and prosperity. What is my greatest regret? I may not be able to say yet. I really wanted, with all my heart, to finish the Oslo peace process, because I believe that if Israel and the\n",
      "\n",
      "when you look back on it, do you regret the substance of what you did? Do you think that going with an employer mandate was the wrong thing? And also, do you regret the detail in which you did it, the fact that you did the 1,300 pages and The President. I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you have any regrets?\"\n",
    "\n",
    "docs = db.similarity_search(query, k=3)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(input_ids,\n",
    "                        max_length=len(input_ids[0])+256,\n",
    "                        # temperature=0.7,\n",
    "                        # num_beams=5,\n",
    "                        repetition_penalty=1.02,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What is your biggest regret?\n",
      "\n",
      "RESPONSE: Well, I regret the way I went forward. No one said anything negative about me this time last year. It turned out to make things differently, politically bad from otherwise, as I have hoped. In retrospect we should probably at least have seen some improvement in our relationship, because I had a lot more confidence than I did early in my term.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is your biggest regret?\"\n",
    "\n",
    "question = f\"QUESTION: {query}\"\n",
    "\n",
    "prompt = \" \".join([doc.page_content for doc in docs]) + \"\\n\\n\" + question + \" RESPONSE:\"\n",
    "\n",
    "output = generate(prompt)[len(prompt):]\n",
    "\n",
    "print(question + \"\\n\\nRESPONSE:\" + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant, that is working nicely given that the model is so small. We can make some small changes here to make everything more compact. We essentially want to define a single Clinton \"Agent\" that we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can define your parameters as a config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os\n",
    "sys.path.append(\"/home/rkd43/Teaching/large-language-models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from agents.rag_agent import RAGAgent\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "model_config = SimpleNamespace(\n",
    "    model_name = 'gpt2-medium',\n",
    "    context_length = 256,\n",
    "    temperature = 0.7,\n",
    "    do_sample = True,\n",
    "    gen_length = 128,\n",
    "    repetition_penalty = 1.1,\n",
    ")\n",
    "\n",
    "training_config = SimpleNamespace(\n",
    "    dataset_path = '../sample_data/cleaned_test_text_1_QA.txt',\n",
    "    context_length = 256,\n",
    "    batch_size = 4,\n",
    "    num_epochs = 20,\n",
    ")\n",
    "\n",
    "database_config = SimpleNamespace(\n",
    "    text_path = '../sample_data/cleaned_test_text_1.txt',\n",
    "    embedding_model = 'sentence-transformers/all-mpnet-base-v2',\n",
    "    chunk_size = 256,\n",
    "    chunk_overlap = 64,\n",
    "    vector_store = FAISS,\n",
    "    text_splitter = CharacterTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initalizing model: gpt2-medium\n",
      "Creating database from: ../sample_data/cleaned_test_text_1.txt\n"
     ]
    }
   ],
   "source": [
    "clinton = Agent(model_config, database_config=database_config, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well first off--I just want everybody to know how sorry I am after having served our country this past eight years as your representative at State Department. Not only were there some very regrettable incidents; even though those things are unfortunate now since last August's elections where many good ideas fell through the cracks or went out of style without being implemented by me personally -- well, obviously these issues cannot go unchallenged indefinitely under Hillary Clinton who has presided over every bad idea from 9/11 to Afghanistan, had no clue whatsoever why Iraq gave weapons-of -mass destruction (WMD) then lied us into invading Libya. My view remains unchanged\n"
     ]
    }
   ],
   "source": [
    "output = clinton.ask_question(\"Do you have any regrets?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton.train(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save agent\n",
    "with open(f'../models/local_clinton_agent_{model_config.model_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(clinton, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know yet. I don't know. I mean, I'm sorry about the Wye River incident. I'm really sorry about that. There should have been more of an investigation. There should have been some kind of public accounting made of what had occurred. And I'm sorry about the fact that the Palestinians, I think, overreacted to it. And I'm sorry about the fact that we gave Arafat and his team a free hand, and that he then went on the offensive, which he did, in a way that was terrible. But the thing I regret most is that I fought that Independent Counsel.\n"
     ]
    }
   ],
   "source": [
    "clinton.model_config.repetition_penalty = 1.01\n",
    "\n",
    "output = clinton.ask_question(\"What is your biggest regret?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the right generation parameters for decent output can be really quite challenging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
