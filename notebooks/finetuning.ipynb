{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning GPT2 with custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'venv (Python 3.10.12)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOSPC: no space left on device, write"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either use `AutoModelWithLMHead` and specify that we want `\"gpt2\"`, or we can use `GPT2LMHeadModel`. The first option is preferable because it means we can use any model with an LM head. We are using GPT-2, the smallest version. There are also `gpt2-medium`, `gpt2-large`, and `gpt2-xl`.\n",
    "\n",
    "**What is an LM Head?**\n",
    "\n",
    "It is the Language Model head. It is the fully connected neural network layer that maps the high-dimensional output of the transformer to the size of the vocabulary used in the model. This part of the network produces the probability distribution over the tokens in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 354.8M parameters\n"
     ]
    }
   ],
   "source": [
    "context_length = 516\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2-medium\").to(\"cuda\")\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is quite small, but still very effective. We write a function that will prompt the model for us. `model.generate` takes a few arguments. Here are the important ones:\n",
    "\n",
    "`max_length`: How many tokens do you want the model to output? If you set this too long, you might get repetition.\n",
    "\n",
    "`temperature`: How random do you want the output to be. 0 is not very random, and 1 is highly random.\n",
    "\n",
    "`no_repeat_ngram_size`: All ngrams of this size can only occur this many times. An ngram is a series of adjacent tokens. So in other words if this is 2, then all ngrams of size 2 can only occur once.\n",
    "\n",
    "`do_sample`: Whether or not to sample. If False, you'll get the same output every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, tokenizer, model, length):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(input_ids,\n",
    "                        max_length=length,\n",
    "                        temperature=0.7,\n",
    "                        num_beams=5,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have any regrets about the Monica Lewinsky scandal?\n",
      "\n",
      "I don't regret it at all. I think it was the right thing to do at the time, and I'm very proud of what I did. But I also think there were other things that I should have done differently. And I've learned a lot from my mistakes.\n"
     ]
    }
   ],
   "source": [
    "output = generate(\"Do you have any regrets about the Monica Lewinsky scandal?\", tokenizer, model, length=256)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above you'll probably get a response from former President Trump. This is perhaps indicative of the training data used to pretrain GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning with custom data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use transcripts of speeches from former President Clinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_text('../sample_data/cleaned_test_text_1_QA_special.txt', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some messing around with the `datasets` library to get this to work. We tokenize the text, cut the text into chunks, and put it into a format the Hugging Face trainer can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 1024)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<start>\",\n",
    "    \"eos_token\": \"<stop>\",\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# resize the token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "{\"input_ids\": outputs.input_ids}\n",
    "\n",
    "tokenized_dataset = Dataset.from_dict({\"input_ids\": outputs.input_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([10, 516])\n",
      "attention_mask shape: torch.Size([10, 516])\n",
      "labels shape: torch.Size([10, 516])\n",
      "\n",
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 274\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_dataset[i] for i in range(10)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "print(f\"\\n{tokenized_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our training arguments: batch size, epochs, etc. All of these things can have an impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='3450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/3450 04:25 < 26:08, 1.88 it/s, Epoch 7.25/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.075000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:424] . unexpected pos 2544239360 vs 2544239252",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m     _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/torch/serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    852\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n\u001b[0;32m--> 853\u001b[0m zip_file\u001b[39m.\u001b[39;49mwrite_record(name, storage\u001b[39m.\u001b[39;49mdata_ptr(), num_bytes)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/67: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/rkd43/Teaching/large-language-models/notebooks/finetuning.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmlatcl-gpu0.cl.cam.ac.uk/home/rkd43/Teaching/large-language-models/notebooks/finetuning.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/transformers/trainer.py:1922\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1920\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1922\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1923\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1924\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/transformers/trainer.py:2282\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2281\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[0;32m-> 2282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[1;32m   2283\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/transformers/trainer.py:2387\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2379\u001b[0m         smp\u001b[39m.\u001b[39msave(\n\u001b[1;32m   2380\u001b[0m             opt_state_dict,\n\u001b[1;32m   2381\u001b[0m             os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, OPTIMIZER_NAME),\n\u001b[1;32m   2382\u001b[0m             partial\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2383\u001b[0m             v3\u001b[39m=\u001b[39msmp\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mshard_optimizer_state,\n\u001b[1;32m   2384\u001b[0m         )\n\u001b[1;32m   2385\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_deepspeed_enabled \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfsdp \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fsdp_enabled):\n\u001b[1;32m   2386\u001b[0m     \u001b[39m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2387\u001b[0m     torch\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstate_dict(), os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(output_dir, OPTIMIZER_NAME))\n\u001b[1;32m   2389\u001b[0m \u001b[39m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   2390\u001b[0m is_deepspeed_custom_scheduler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_deepspeed_enabled \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   2391\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   2392\u001b[0m )\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Teaching/large-language-models/venv/lib/python3.10/site-packages/torch/serialization.py:466\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mwrite_end_of_file()\n\u001b[1;32m    467\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:424] . unexpected pos 2544239360 vs 2544239252"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n",
      "Do you have any regrets about the Monica Lewinsky scandal?\n",
      "\n",
      " I mean, do you?No, I don't. I think it was the best thing that ever happened to American politics, and I hope it never happens to anybody else. But I do regret the fact that I didn't do more to help her through the Whitewater investigation or the Independent Counsel's investigation. And I regret that, in retrospect, it may have been the right thing to do, because of the special relationship that existed between her and me and Bill Clinton, who was at the time Governor of Arkansas and a Senator from New York. It was very important to me to have a relationship with her that was mutually respectful and respectful of her position in the family life and the private lives of both of us. That's not the way I would have handled it, but it worked out just fine for me. So I'm not going to dwell on it. On the other hand, if I had done something different, or if there had been some other event that occurred that led to the appointment of Janet Reno, for example, where there was some controversy about her having an improper financial interest in a real estate deal involving her husband's family, that would not be appropriate for the attorney general.\n"
     ]
    }
   ],
   "source": [
    "# prompt the model with a query and get the answer\n",
    "prompt = \"Do you have any regrets about the Monica Lewinsky scandal?\"\n",
    "output = generate(prompt, tokenizer, model, 256)[len(prompt):]\n",
    "\n",
    "print(prompt + \"\\n\\n\" + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is working decently. Now let's incorporate RAG..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model(\"../models/qa_model/\")\n",
    "\n",
    "# clear gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/qa_model/\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"../models/qa_model/\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(text_path, chunk_size, chunk_overlap):\n",
    "    # load text\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split text\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=' ',\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "    db = FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../sample_data/cleaned_test_text_1_QA_special.txt'\n",
    "db = create_database(text_path, chunk_size=300, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question on a matter of history that I feel compelled to ask you, Mr. President. We sat, you and I, 2 years ago almost to the day, and I it was the day that the Monica Lewinsky story broke in the Washington Post and the Los Angeles Times. And I and you denied that you had had an improper sexual\n",
      "\n",
      "one mistake. I apologized for it. I paid a high price for it, and I've done my best to atone for it by being a good President. But I believe we also endured what history will clearly record was a bogus investigation, where there was nothing to Whitewater and nothing to these other charges, and they\n",
      "\n",
      "Washington Post and the Los Angeles Times. And I and you denied that you had had an improper sexual relationship with Ms. Lewinsky. In retrospect, if you had answered that differently right at the beginning, not only just my question but all those questions at the beginning, do you think there would\n",
      "\n",
      "David Gergen agreed and said you should turn over all the data, everything. And you didn't do it. Do you regret that? Do you think that that changed things?<stop><start>I don't believe, given the subsequent coverage of the Whitewater thing, it would have made any difference. What I regret is asking\n",
      "\n",
      "standing up. Laughter<stop><start>What was the opportunity cost of that scandal? What did it cost you?<stop><start>I don't know yet, because actually we did in '98 we won seats in the House of Representatives, the first time a President's party has done that since<stop><start>I mean,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you have any regrets about the Monica Lewinsky scandal?\"\n",
    "\n",
    "docs = db.similarity_search(query, k=5)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have any regrets about the Monica Lewinsky scandal?\n",
      "\n",
      "Well, I regret it. I think it was a terrible mistake on my part to characterize it as something that had nothing to do with my husband and his wife and their children. And I'm glad that the facts were out there, and I hope the American people will come forward and find out whether there was anything wrong with it and, if so, whether anybody else should be punished or not. But I am profoundly sorry that it occurred to you and your family and to the country at large, because I know that there are people in this country who believe that something terrible had happened and ought to be brought to light. That's what I want people to think about when they see a jury of law abiding citizens come back and say, \"This woman did something wrong. She should never have been in office.\" I don't think that that's appropriate. It's not appropriate for me to say that I condone or even in the strongest of terms possible refer to or condone, in any way, the conduct of an elected official or a family member who is involved in a serious, public way in trying to hurt another person or to advance an agenda that is designed to rile up a particular group of people\n"
     ]
    }
   ],
   "source": [
    "output = generate(prompt, tokenizer, model, 256)[len(prompt):]\n",
    "\n",
    "print(query + \"\\n\\n\" + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
