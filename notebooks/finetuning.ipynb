{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning GPT-2 with custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either use `AutoModelWithLMHead` and specify that we want `\"gpt2\"`, or we can use `GPT2LMHeadModel`. The first option is preferable because it means we can use any model with an LM head. We are using GPT-2, the smallest version. There are also `gpt2-medium`, `gpt2-large`, and `gpt2-xl`.\n",
    "\n",
    "**What is an LM Head?**\n",
    "\n",
    "It is the Language Model head. It is the fully connected neural network layer that maps the high-dimensional output of the transformer to the size of the vocabulary used in the model. This part of the network produces the probability distribution over the tokens in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "context_length = 256\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is quite small, but still very effective. We write a function that will prompt the model for us. `model.generate` takes a few arguments. Here are the important ones:\n",
    "\n",
    "`max_length`: How many tokens do you want the model to output? If you set this too long, you might get repetition.\n",
    "\n",
    "`temperature`: How random do you want the output to be. 0 is not very random, and 1 is highly random.\n",
    "\n",
    "`no_repeat_ngram_size`: All ngrams of this size can only occur this many times. An ngram is a series of adjacent tokens. So in other words if this is 2, then all ngrams of size 2 can only occur once.\n",
    "\n",
    "`do_sample`: Whether or not to sample. If False, you'll get the same output every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have any regrets?\n",
      "\n",
      "I don't regret anything. I'm just happy that I was able to do what I wanted to. That's all.\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(input_ids,\n",
    "                        max_length=256,\n",
    "                        # temperature=0.7,\n",
    "                        num_beams=5,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        early_stopping=True,\n",
    "                        # do_sample=True,\n",
    "                        # pad_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "output = generate(\"Do you have any regrets?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above you'll probably get a coherent, but meaningless response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further pretraining on a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use transcripts of press events from former President Clinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_text('../sample_data/cleaned_test_text_1.txt', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some messing around with the `datasets` library to get this to work. We tokenize the text, cut the text into chunks, and put it into a format the Hugging Face trainer can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "{\"input_ids\": outputs.input_ids}\n",
    "\n",
    "tokenized_dataset = Dataset.from_dict({\"input_ids\": outputs.input_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 256])\n",
      "attention_mask shape: torch.Size([5, 256])\n",
      "labels shape: torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = 'tokenizer.eos_token'\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "out = data_collator([tokenized_dataset[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1400 04:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.889000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.873000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1400, training_loss=2.2741852133614677, metrics={'train_runtime': 257.7111, 'train_samples_per_second': 43.382, 'train_steps_per_second': 5.432, 'total_flos': 1460622458880000.0, 'train_loss': 2.2741852133614677, 'epoch': 20.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have any regrets? The President. Well, I don't have to regret anything. I'm very proud of what we did in Vietnam, and I regret that we didn't do it the way we wanted it to be done. But I think the mistake I made was not doing what I thought was right, which led to the mistakes we made in the first Gulf war, in which we lost more than 100,000 Americans and hundreds of our friends and allies. And we've done a lot of good things since then. We've helped the people of Vietnam fight the Viet Cong, the North Vietnamese Army and the Republic of Korea and other countries that are involved in our efforts to end the violence there and restore democracy and respect the Line of Control. Mr. Wenner. Do you ever get angry at people who say things like, \"He killed his father? He murdered his mother?\" or \"Did he get away with it? Didn't he do what he was charged with? Did he lose his family?\" That's the kind of anger I have for the Vietnam veterans, because I know they say the same things to me, but they're not supposed to say it. They shouldn't be punished for what they did. The American people deserve to know the truth\n"
     ]
    }
   ],
   "source": [
    "output = generate(\"Do you have any regrets?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model(\"../models/basic_model/\")\n",
    "\n",
    "# clear gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is definitely learning the data. Now let's see if we can make it learn a general QA format. To do this, we recognize some of the reoccuring features of the data set, such as responses being given by `The President.` We look for all instances of `The President.` and replace with `RESPONSE: `. We also search for names of the interviewers and replace with `QUESTION: `. We also include the end of sentence token at the end of the responses, in the hopes that it will also learn these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5761.41it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1093.98it/s]\n",
      "Generating train split: 1 examples [00:00, 170.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = Dataset.from_text('../sample_data/cleaned_test_text_1_QA.txt', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "context_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "  model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "{\"input_ids\": outputs.input_ids}\n",
    "\n",
    "tokenized_dataset = Dataset.from_dict({\"input_ids\": outputs.input_ids})\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "out = data_collator([tokenized_dataset[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: We understand you made a foreign policy related call shortly. RESPONSE: Yes, I just talked to President Kim about the No Gun Ri incident and personally expressed my regret to him. And I thanked him for the work that we had done together in developing our mutual statement. We also set up this scholarship fund and did some other things that we hope will be a genuine gesture of our regret. It was a very you know, I had a good talk with him.<|endoftext|>QUESTION: Any particular reason why you used the word \"regret\" instead of \"apology\" in your statement? RESPONSE: I think the findings were I think he knows that \"regret\" and \"apology\" both mean the same thing, in terms of being profoundly sorry for what happened. But I believe that the people who looked into it could not conclude that there was a deliberate act, decided at a high enough level in the military hierarchy, to acknowledge that, in effect, the Government had participated in something that was terrible. So I don't think there's any difference in the two words, on a human level, because we are profoundly sorry that it happened and sorry that any Americans were involved in it. But I think that in terms of the kind of responsibility the institution of the military that the facts were sufficiently unclear after all this time that the people who were reviewing it thought it was the appropriate language. And we worked it out with the Koreans and obviously shared whatever we could find with them. These people have been our friends for 50 years. We didn't have I told our guys to play it straight, that we didn't have an interest in trying to cover anything up or sugar coat anything we needed to try to get to the bottom of this. I think that we've done about the best we can do. And I hope that the people of Korea will accept our statement as genuine, and I hope it will bring some solace to the family members and the few people that still survived who were involved in it, who will never get over it.<|endoftext|>QUESTION: Let me ask you another topical question. California is on the verge of blackouts. Is there anything you can do in your remaining time in office? RESPONSE: Well, I'm working at it. We have done some things. Secretary Richardson has worked very hard to make sure that the wholesalers kept selling the power to the utilities. But essentially, what happened was before without any involvement from the Federal Government and before the previous administration in\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out['input_ids'][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really the process is just identical to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 04:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.245600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=2.5193115670340402, metrics={'train_runtime': 255.9539, 'train_samples_per_second': 21.801, 'train_steps_per_second': 2.735, 'total_flos': 1458009538560000.0, 'train_loss': 2.5193115670340402, 'epoch': 20.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We slightly change our prompt and display templates, just to make it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(input_ids,\n",
    "                        max_length=len(input_ids[0])+128,\n",
    "                        # temperature=0.7,\n",
    "                        num_beams=5,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Do you have any regrets?\n",
      "\n",
      "RESPONSE: No.\n"
     ]
    }
   ],
   "source": [
    "question = \"QUESTION: Do you have any regrets?\"\n",
    "prompt = f\"{question} RESPONSE:\"\n",
    "output = generate(prompt)[len(prompt):]\n",
    "print(f\"{question}\\n\\nRESPONSE:{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QUESTION: What did you talk to President Kim about today? RESPONSE: Well, I talked to him about the missile program and the North Korea issue. And I said, \"Mr. President, we\\'re going to have to work together to try to develop a long term strategy to stop this nuclear threat, and we\\'ve got to find a way to put it in place that will stop it.\" And he was very supportive of that. So I thought it was a very good thing to say.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"QUESTION: What did you talk to President Kim about today? RESPONSE:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model(\"../models/QA_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is working somewhat. Note that GPT-2 is a very small model, and the dataset is also small. In general, the results will be quite poor. You can try rerunning this on `gpt-medium` or `gpt-large` if you have the compute and memory. On my machine, this whole notebook will consume about 12GB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreival Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(text_path, chunk_size, chunk_overlap):\n",
    "    # load text\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split text\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=' ',\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "    db = FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "    return db\n",
    "\n",
    "text_path = '../sample_data/cleaned_test_text_1_QA.txt'\n",
    "db = create_database(text_path, chunk_size=256, chunk_overlap=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is my greatest regret? I may not be able to say yet. I really wanted, with all my heart, to finish the Oslo peace process, because I believe that if Israel and the Palestinians could be reconciled, first the State of Israel would be secure, which is very\n",
      "\n",
      "I wanted to do, but the overwhelming majority of things I wanted to do I was able to accomplish, and I'm grateful that it worked out for the country. And then a lot of other things came up along the way which were good for the country. So I'm happy now,\n",
      "\n",
      "our people stuck with me, and that the American people stuck with me, and I was able to resist what it was they attempted to do. But I do regret the fact that I wasn't straight with the American people about it. It was something I was ashamed of and pained\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you have any regrets?\"\n",
    "\n",
    "docs = db.similarity_search(query, k=3)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(input_ids,\n",
    "                        max_length=len(input_ids[0])+256,\n",
    "                        temperature=0.7,\n",
    "                        num_beams=5,\n",
    "                        no_repeat_ngram_size=3,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Can you tell us about the Whitewater scandal?\n",
      "\n",
      "RESPONSE: I can tell you, first of all, I regret that I didn't do what I said I would do. I did what I thought was the right thing, but I did it in the wrong way. And I'm sorry about that. I think it's important that we all remember what happened to me. I was sitting in the Oval Office in the White House when I met with Mr. Arafat, and he said to me, \"Mr. President, you've got to do this.\" And I said, \"I don't want to do it. I don't think I can do it.\"\n"
     ]
    }
   ],
   "source": [
    "question = \"QUESTION: Can you tell us about the Whitewater scandal?\"\n",
    "\n",
    "prompt = \" \".join([doc.page_content for doc in docs]) + \"\\n\\n\" + question + \" RESPONSE:\"\n",
    "\n",
    "output = generate(prompt)[len(prompt):]\n",
    "\n",
    "print(question + \"\\n\\nRESPONSE:\" + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant, that is working nicely given that the model is so small. We can make some small changes here to make everything more compact. We essentially want to define a single Clinton \"Agent\" that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, model_config : SimpleNamespace, database_config : SimpleNamespace = None):\n",
    "        \"\"\"RAG agent\n",
    "\n",
    "        Args:\n",
    "            model_config (SimpleNamespace): model parameters\n",
    "            database_config (SimpleNamespace, optional): database parameters. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.model_config = model_config\n",
    "        self._validate_model_config()\n",
    "        self.database_config = database_config\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        print(f\"Initalizing model: {self.model_config.model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_config.model_name)\n",
    "        self.model = AutoModelWithLMHead.from_pretrained(self.model_config.model_name).to(self.device)\n",
    "\n",
    "        if self.database_config is not None:\n",
    "            print(f\"Creating database from: {self.database_config.text_path}\")\n",
    "            self.db = self._create_database()\n",
    "\n",
    "        self.trained = False\n",
    "\n",
    "    \n",
    "    def __repr__(self):\n",
    "        agent_config = f\"self.model_config: {self.model_config}\\nself.database_config: {self.database_config}\"\n",
    "        return agent_config\n",
    "\n",
    "\n",
    "    def _validate_model_config(self):\n",
    "        assert hasattr(self.model_config, 'model_name'), \"model_config must have a model_name attribute\"\n",
    "        if not hasattr(self.model_config, 'gen_length'):\n",
    "            self.model_config.gen_length = 128\n",
    "        if not hasattr(self.model_config, 'context_length'):\n",
    "            self.model_config.context_length = 256\n",
    "        if not hasattr(self.model_config, 'temperature'):\n",
    "            self.model_config.temperature = 0.7\n",
    "        if not hasattr(self.model_config, 'do_sample'):\n",
    "            self.model_config.do_sample = True\n",
    "\n",
    "    \n",
    "    def _validate_database_config(self):\n",
    "        assert hasattr(self.database_config, 'text_path'), \"database_config must have a text_path attribute\"\n",
    "        assert hasattr(self.database_config, 'text_splitter'), \"database_config must have a text_splitter attribute\"\n",
    "        assert hasattr(self.database_config, 'chunk_size'), \"database_config must have a chunk_size attribute\"\n",
    "        assert hasattr(self.database_config, 'chunk_overlap'), \"database_config must have a chunk_overlap attribute\"\n",
    "        assert hasattr(self.database_config, 'embedding_model'), \"database_config must have an embedding_model attribute\"\n",
    "        assert hasattr(self.database_config, 'vector_store'), \"database_config must have a vector_store attribute\"\n",
    "            \n",
    "    \n",
    "    def _create_database(self):\n",
    "        self._validate_database_config()\n",
    "        \n",
    "        with open(self.database_config.text_path, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Split text\n",
    "        text_splitter = self.database_config.text_splitter(\n",
    "            separator=' ',\n",
    "            chunk_size=self.database_config.chunk_size,\n",
    "            chunk_overlap=self.database_config.chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "        chunks = text_splitter.split_text(text)\n",
    "\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=self.database_config.embedding_model)\n",
    "        db = self.database_config.vector_store.from_texts(chunks, embeddings)\n",
    "\n",
    "        return db\n",
    "\n",
    "\n",
    "    def ask_question(self, query : str = \"What is your name?\", retrieval : bool = True) -> str:\n",
    "        \"\"\"Ask a question\n",
    "\n",
    "        Args:\n",
    "            query (str, optional): Query to the Agent. Defaults to \"What is your name?\".\n",
    "\n",
    "        Returns:\n",
    "            str: Output string from the Agent\n",
    "        \"\"\"\n",
    "        question = \"QUESTION: \" + query\n",
    "\n",
    "        if retrieval and self.database_config is not None:\n",
    "            docs = self.db.similarity_search(query, k=3)\n",
    "            prompt = \" \".join([doc.page_content for doc in docs]) + \"\\n\\n\" + question + \" RESPONSE:\"\n",
    "        else:\n",
    "            prompt = question + \" RESPONSE:\"\n",
    "\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "\n",
    "        output = self.model.generate(input_ids,\n",
    "                        max_length=self.model_config.gen_length + len(input_ids[0]),\n",
    "                        # temperature=0.7,\n",
    "                        num_beams=5,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        early_stopping=True,\n",
    "                        # do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id   \n",
    "                    )\n",
    "\n",
    "        # output without input_ids\n",
    "        return self.tokenizer.decode(output[0][len(input_ids[0]):], skip_special_tokens=True)[1:]\n",
    "\n",
    "\n",
    "    def train(self, training_config : SimpleNamespace) -> None:\n",
    "        \"\"\"Train the Agent using the given training_config\n",
    "\n",
    "        Args:\n",
    "            training_config (SimpleNamespace): Training hyperparameters\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "            with torch.no_grad():\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        data = Dataset.from_text(training_config.dataset_path, split='train')\n",
    "        \n",
    "        outputs = self.tokenizer(\n",
    "                data[\"text\"],\n",
    "                truncation=True,\n",
    "                max_length=training_config.context_length,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_length=True,\n",
    "            )\n",
    "\n",
    "        {\"input_ids\": outputs.input_ids}\n",
    "\n",
    "        tokenized_dataset = Dataset.from_dict({\"input_ids\": outputs.input_ids})\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            per_device_train_batch_size=training_config.batch_size,\n",
    "            num_train_epochs=training_config.num_epochs,\n",
    "            logging_steps=100\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenized_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        self.trained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can define your parameters as a config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "model_config = SimpleNamespace(\n",
    "    model_name = 'gpt2-medium',\n",
    "    context_length = 256,\n",
    "    temperature = 0.7,\n",
    "    do_sample = True,\n",
    "    gen_length = 128,\n",
    ")\n",
    "\n",
    "training_config = SimpleNamespace(\n",
    "    dataset_path = '../sample_data/cleaned_test_text_1_QA.txt',\n",
    "    context_length = 256,\n",
    "    batch_size = 4,\n",
    "    num_epochs = 20,\n",
    ")\n",
    "\n",
    "database_config = SimpleNamespace(\n",
    "    text_path = '../sample_data/cleaned_test_text_1_QA.txt',\n",
    "    embedding_model = 'sentence-transformers/all-mpnet-base-v2',\n",
    "    chunk_size = 256,\n",
    "    chunk_overlap = 64,\n",
    "    vector_store = FAISS,\n",
    "    text_splitter = CharacterTextSplitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton = Agent(model_config, database_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do. I've read a lot of books. I don't know if I've ever read a book before, but I do like to read. I love to read, and I love reading. I'm a big fan of the New York Times bestseller, The Great American Novel, which is a great book. It's about a young man who finds himself in the middle of a war, and he finds out that his father is dead, and that his mother is dead. And he has to find out what's going on, and what's happening in his life. And it's a great story, and the book is\n"
     ]
    }
   ],
   "source": [
    "output = clinton.ask_question(\"Do you have any favorite websites?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2800' max='2800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2800/2800 12:30, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.768800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.869500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.806600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.491300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.444900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.411800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clinton.train(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't think it's very good, for a number of reasons. One is that, since 1800, every President since has been elected by a narrow margin, unless you happen to be a Governor, a Senator, or the President of the United States. So if you're a one point person and you win by five or six tenths of one percent, well, that's not going to bother anybody. Now, you've got to have a margin of victory somewhere between one and two percent to make sure that your votes do not unduly prejudge the outcome of an election. Also, the process is designed to prevent this.\n"
     ]
    }
   ],
   "source": [
    "output = clinton.ask_question(\"What do you think about the electoral college?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
