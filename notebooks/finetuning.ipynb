{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning GPT-2 with custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either use `AutoModelWithLMHead` and specify that we want `\"gpt2\"`, or we can use `GPT2LMHeadModel`. The first option is preferable because it means we can use any model with an LM head. We are using GPT-2, the smallest version. There are also `gpt2-medium`, `gpt2-large`, and `gpt2-xl`.\n",
    "\n",
    "**What is an LM Head?**\n",
    "\n",
    "It is the Language Model head. It is the fully connected neural network layer that maps the high-dimensional output of the transformer to the size of the vocabulary used in the model. This part of the network produces the probability distribution over the tokens in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "context_length = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is quite small, but still very effective. We write a function that will prompt the model for us. `model.generate` takes a few arguments. Here are the important ones:\n",
    "\n",
    "`max_length`: How many tokens do you want the model to output? If you set this too long, you might get repetition.\n",
    "\n",
    "`temperature`: How random do you want the output to be. 0 is not very random, and 1 is highly random.\n",
    "\n",
    "`no_repeat_ngram_size`: All ngrams of this size can only occur this many times. An ngram is a series of adjacent tokens. So in other words if this is 2, then all ngrams of size 2 can only occur once.\n",
    "\n",
    "`do_sample`: Whether or not to sample. If False, you'll get the same output every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, tokenizer, model, length):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(input_ids,\n",
    "                        max_length=length,\n",
    "                        temperature=0.7,\n",
    "                        num_beams=5,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State your name and occupation.\n",
      "\n",
      "If you have any questions or concerns, please contact us.\n"
     ]
    }
   ],
   "source": [
    "output = generate(\"State your name and occupation.\", tokenizer, model, length=256)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above you'll probably get a response from former President Trump. This is perhaps indicative of the training data used to pretrain GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning with custom data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use transcripts of speeches from former President Clinton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_text('../sample_data/cleaned_test_text_1_QA_special.txt', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some messing around with the `datasets` library to get this to work. We tokenize the text, cut the text into chunks, and put it into a format the Hugging Face trainer can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<start>\",\n",
    "    \"eos_token\": \"<stop>\",\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# # resize the token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "        data[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "{\"input_ids\": outputs.input_ids}\n",
    "\n",
    "tokenized_dataset = Dataset.from_dict({\"input_ids\": outputs.input_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([10, 512])\n",
      "attention_mask shape: torch.Size([10, 512])\n",
      "labels shape: torch.Size([10, 512])\n",
      "\n",
      "Dataset({\n",
      "    features: ['input_ids'],\n",
      "    num_rows: 276\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_dataset[i] for i in range(10)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "print(f\"\\n{tokenized_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our training arguments: batch size, epochs, etc. All of these things can have an impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    per_device_train_batch_size=12,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"no\"\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 04:00, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.529900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=460, training_loss=3.334866299836532, metrics={'train_runtime': 241.5265, 'train_samples_per_second': 22.855, 'train_steps_per_second': 1.905, 'total_flos': 1442332016640000.0, 'train_loss': 3.334866299836532, 'epoch': 20.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have any regrets about your life?\n",
      "\n",
      "Well, I don't regret it. I think it was a good thing for the country, and I regret that I didn't do anything wrong. But I'm not sure I ever would have been President if I hadn't been able to do what I did. And I've tried to follow the law and the Constitution, but I never had any kind of personal involvement in the decisionmaking process. So I can't comment on that. It's just a question of whether I should have done something wrong or not. If I had done it wrong, it would be a huge mistake for me, because I knew I was making a mistake. You know, the thing that happened to me in '94 was basically the beginning of the end of my political career, which was when I got out of college and started running for President in high school. We had a very bad year. There were a lot of things that we did wrong that weren't good for our country and bad for America. That's why I thought I ought to be President, not just because it's a big mistake I made to get caught up in politics and try to live my life the way I wanted to.What do you think of\n"
     ]
    }
   ],
   "source": [
    "# prompt the model with a query and get the answer\n",
    "prompt = \"Do you have any regrets about your life?\"\n",
    "output = generate(prompt, tokenizer, model, 256)[len(prompt):]\n",
    "\n",
    "print(prompt + \"\\n\\n\" + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is working decently. Now let's incorporate RAG..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model(\"../models/qa_model/\")\n",
    "\n",
    "# clear gpu memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/qa_model/\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"../models/qa_model/\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(text_path, chunk_size, chunk_overlap):\n",
    "    # load text\n",
    "    with open(text_path, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split text\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=' ',\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "    db = FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '../sample_data/cleaned_test_text_1.txt'\n",
    "db = create_database(text_path, chunk_size=512, chunk_overlap=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I had made a terrible personal mistake, which I did try to correct, which then a year later got outed on or almost a year later and had to live with. And it caused an enormous amount of pain to my family and my administration and to the country at large, and I felt awful about it. And I had to deal with the aftermath of it. And then, I had to deal with what the Republicans were trying to do with it. But I had a totally different take on it than most people. I really believed then and I believe now I was\n",
      "\n",
      "for that. And I will leave office with that sense of gratitude, because I think that's what every President wants to do. Every President wants to feel that during his tenure of service, America grew stronger and healthier and better. I feel good about where we are in our relations with the rest of the world. I think we've basically been a force for peace and prosperity. What is my greatest regret? I may not be able to say yet. I really wanted, with all my heart, to finish the Oslo peace process, because I\n",
      "\n",
      "that I fought the Independent Counsel. And what they did was, in that case and generally, was completely overboard. And now rational retrospectives are beginning to come out, where people have no connection to me, talking about what an abuse of power it was and what a threat to the American system it was. And I'm glad that our people stuck with me, and that the American people stuck with me, and I was able to resist what it was they attempted to do. But I do regret the fact that I wasn't straight with the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Do you have any regrets about your life?\"\n",
    "\n",
    "docs = db.similarity_search(query, k=3)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have any regrets about your life?\n",
      "\n",
      "Well, first of all I regret not having done more for the United States in the Middle East and the Balkans and all the other places I've been involved in, including Bosnia, Kosovo, Rwanda, Bosnia and Kosovo. It was a mistake I made at some point, but I didn't regret it as much as I would have liked to have done if it hadn't been for what happened in Bosnia. So I don't think I should have made that mistake again. Secondly, it's important for me to remember that when I became President, there were a lot of people out there who believed that Saddam Hussein had weapons of mass destruction and chemical weapons and biological weapons. That's not true. The truth is, the truth was that there was no such thing as a chemical or biological weapon. There was only a small percentage of\n"
     ]
    }
   ],
   "source": [
    "prompt = \" \".join([doc.page_content for doc in docs]) + \"\\n\\n\" + query\n",
    "\n",
    "output = generate(prompt, tokenizer, model, 512)[len(prompt):]\n",
    "\n",
    "print(query + \"\\n\\n\" + output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
