{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you might be wondering is how we can evaluate the RAG process. Well, it's hard. There are a few possible techniques we can use. And here we will demonstrate a few here:\n",
    "\n",
    "- Semantic similarity\n",
    "\n",
    "- Faithfulness\n",
    "\n",
    "The core of these methods (any many methods that evaluate RAG systems) involves feeding the entire paper into an LLM and asking it to generate some questions and some answers based on the paper. We can then assess things like semantic similarity. We can also ask the model to evaluate whether the answer it gave can actually be inferred from the context given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import fitz\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "from typing import Any\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same approach as previous notebook. So we have moved a bunch of our code into a `utils.py` file. We have mostly kept things the same, but have a look over it and make sure you understand how it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import chunker, DocumentDB, load_template\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"data/paper.pdf\")\n",
    "text_chunks, doc_idxs = chunker(chunk_size=1024, overlap=128, documents=documents)\n",
    "\n",
    "# we can do this because we have secretly called the \n",
    "doc_db = DocumentDB(\"paper_db\", path=\"../data-storage-and-ingestion/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['chunk_30', 'chunk_12']],\n",
       " 'distances': [[0.7026067185204117, 0.7119312067010161]],\n",
       " 'metadatas': [[{'doc_idx': 27}, {'doc_idx': 11}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['A Philosophical Introduction to Language Models\\nPart I\\nQiu, L., Shaw, P., Pasupat, P., Nowak, P., Linzen, T., Sha, F. & Toutanova, K. (2022), Improving\\nCompositional Generalization with Latent Structure and Data Augmentation, in ‘Proceedings of the\\n2022 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies’, Association for Computational Linguistics, Seattle, United States,\\npp. 4341–4362.\\nQuilty-Dunn, J., Porot, N. & Mandelbaum, E. (2022), ‘The Best Game in Town: The Re-Emergence of\\nthe Language of Thought Hypothesis Across the Cognitive Sciences’, Behavioral and Brain Sciences\\npp. 1–55.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. & Liu, P. J. (2020),\\n‘Exploring the limits of transfer learning with a unified text-to-text transformer’, The Journal of\\nMachine Learning Research 21(1), 140:5485–140:5551.\\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. (2022), ‘Hierarchical Text-Conditional Image\\nGeneration with CLIP Latents’.\\nSalton, G., Wong, A. & Yang, C. S. (1975), ‘A vector space model for automatic indexing’, Communica-\\ntions of the ACM 18(11), 613–620.\\nSavelka, J., Agarwal, A., An, M., Bogart, C. & Sakr, M. (2023), Thrilled by Your Progress! Large\\nLanguage Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Program-\\nming Courses, in ‘Proceedings of the 2023 ACM Conference on International Computing Education\\nResearch V.1’, pp. 78–92.\\nSavelka, J., Ashley, K. D., Gray, M. A., Westermann, H. & Xu, H. (2023), Can GPT-4 Support Analysis\\nof Textual Data in Tasks Requiring Highly Specialized Domain Expertise?, in ‘Proceedings of the\\n2023 Conference on Innovation and Technology in Computer Science Education V. 1’, pp. 117–123.\\nSchmidhuber, J. (1990), Towards Compositional Learning with Dynamic Neural Networks, Inst. für\\nInformatik.\\nSchut, L., Tomasev, N., McGrath, T., Hassabis, D., Paquet, U. & Kim, B. (2023), ‘Bridging the Human-AI\\nKnowledge Gap: Concept Discovery and Transfer in AlphaZero’.\\nSearle, J. R. (1980), ‘Minds, Brains, and Programs’, Behavioral and Brain Sciences 3(3), 417–57.\\nShinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K. & Yao, S. (2023), ‘Reflexion:\\nLanguage Agents with Verbal Reinforcement Learning’.\\nSmolensky, P. (1988), ‘On the proper treatment of connectionism’, Behavioral and Brain Sciences\\n11(1), 1–23.\\nSmolensky, P. (1989), Connectionism and Constituent Structure, in R. Pfeifer, Z. Schreter, F. Fogelman-\\nSoulié & L. Steels, eds, ‘Connectionism in Perspective’, Elsevier.\\nSmolensky, P., McCoy, R., Fernandez, R., Goldrick, M. & Gao, J. (2022a), ‘Neurocompositional\\nComputing: From the Central Paradox of Cognition to a New Generation of AI Systems’, AI\\nMagazine 43(3), 308–322.\\nSmolensky, P., McCoy, R. T., Fernandez, R., Goldrick, M. & Gao, J. (2022b), ‘Neurocompositional\\ncomputing in human and machine intelligence: A tutorial’.\\nSober, E. (1998), Morgan’s canon, in ‘The Evolution of Mind’, Oxford University Press, New York, NY,\\nUS, pp. 224–242.\\n28',\n",
       "   'A Philosophical Introduction to Language Models\\nPart I\\nInitial DNN performance on SCAN and other synthetic datasets probing compositional gener-\\nalization – such as CFQ (Keysers et al. 2019) and COGS (Kim & Linzen 2020) – was somewhat\\nunderwhelming. Testing generally revealed a significant gap between performance on the train set\\nand on the test set, suggesting a failure to properly generalize across syntactic distribution shifts.\\nSince then, however, many Transformer-based models have achieved good to perfect accuracy on these\\ntests. This progress was enabled by various strategies, including tweaks to the vanilla Transformer\\narchitecture to provide more effective inductive biases (Csordás et al. 2022, Ontanon et al. 2022) and\\ndata augmentation to help models learn the right kind of structure (Andreas 2020, Akyürek et al.\\n2020, Qiu et al. 2022).\\nMeta-learning, or learning to learn better by generalizing from exposure to many related learning\\ntasks (Conklin et al. 2021, Lake & Baroni 2023), has also shown promise without further architectural\\ntweaks. Standard supervised learning rests on the assumption that training and testing data are\\ndrawn from the same distribution, which can lead models to “overfit” to the training data and fail to\\ngeneralize to the testing data. Meta-learning exposes models to several distributions of related tasks,\\nin order to promote acquisition of generalizable knowledge. For example, Lake & Baroni (2023) show\\nthat a standard Transformer-based neural network, when trained on a stream of distinct artificial\\ntasks, can achieve systematic generalization in a controlled few-shot learning experiment, as well as\\nstate-of-the-art performance on systematic generalization benchmarks. At test time, the model exhibits\\nhuman-like accuracy and error patterns, all without explicit compositional rules. While meta-learning\\nacross various tasks helps promote compositional generalization, recent work suggests that merely\\nextending the standard training of a network beyond the point of achieving high accuracy on training\\ndata can lead it to develop more tree-structured computations and generalize significantly better to\\nheld-out test data that require learning hierarchical rules (Murty et al. 2023). The achievements of\\nTransformer models on compositional generalization benchmarks provide tentative evidence that\\nbuilt-in rigid compositional rules may not be needed to emulate the structure-sensitive operations of\\ncognition.\\nOne interpretation of these results is that, given the right architecture, learning objective, and\\ntraining data, ANNs might achieve human-like compositional generalization by implementing a\\nlanguage of thought architecture – in accordance with the second horn of the classicist dilemma\\n(Quilty-Dunn et al. 2022, Pavlick 2023). But an alternative interpretation is available, on which ANNs\\ncan achieve compositional generalization with non-classical constituent structure and composition\\nfunctions. Behavioral evidence alone is insufficient to arbitrate between these two hypotheses.8 But\\nit is also worth noting that the exact requirements for implementing a language of thought are still\\nsubject to debate (Smolensky 1989, McGrath et al. 2023).\\nOn the traditional Fodorian view, mental processes operate on discrete symbolic representations\\nwith semantic and syntactic structure, such that syntactic constituents are inherently semantically\\nevaluable and play direct causal roles in cognitive processing. By contrast, the continuous vectors that\\nbear semantic interpretation in ANNs are taken to lack discrete, semantically evaluable constituents\\nthat participate in processing at the algorithmic level, which operates on lower-level activation values\\ninstead. This raises the question whether the abstracted descriptions of stable patterns observed\\nin the aggregate behavior of ANNs’ lower-level mechanisms can fulfill the requirements of classical\\nconstituent structure, especially when their direct causal efficacy in processing is not transparent.\\nFor proponents of connectionism who argue that ANNs may offer a non-classical path to modeling\\ncognitive structure, this is a feature rather than a bug. Indeed, classical models likely make overly\\nrigid assumptions about representational formats, binding mechanisms, algorithmic transparency,\\nand demands for systematicity; conversely, even modern ANNs likely fail to implement their specific\\n8See Part II for a brief discussion of mechanistic evidence in favor of the second hypothesis.\\n12']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents', 'distances']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_db.query_db(\"Abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate question answer pairs\n",
    "For this, we will use `gpt-4o` because we want high quality question answer pairs. Ideally, you would do this with humans - subject matter experts would carefully hand-craft these pairs.\n",
    "\n",
    "The first stage is to then generate 10 Q&A pairs using pydantic again. The implementations presented here closely follow the method used by the [RAGAS](https://docs.ragas.io/en/stable/getstarted/index.html#get-started) library.\n",
    "\n",
    "We implement a Pydantic BaseModel class that will house our list of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'questions': {'items': {'type': 'string'}, 'title': 'List of questions', 'type': 'array'}, 'answers': {'items': {'type': 'string'}, 'title': 'List of answers', 'type': 'array'}}, 'required': ['questions', 'answers'], 'title': 'QAPairs', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "class QAPairs(BaseModel):\n",
    "    questions: list[str] = Field(..., title=\"List of questions\")\n",
    "    answers: list[str] = Field(..., title=\"List of answers\")\n",
    "\n",
    "print(QAPairs.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a prompt that we can use to generate these Q&A pairs. It looks something like this:\n",
    "\n",
    "---\n",
    "```\n",
    "You are a reading comprehension system that is an expert at extracting information from academic papers.\n",
    "Your task is to carefully read the provided text \"CONTEXT\" and then generate question and answer pairs.\n",
    "Your questions should be concise. Your answers should be as detailed as possible, including any mathematical or numerical results from the text.\n",
    "You should aim to produce approximately one paragraph for your answers (100-200 words).\n",
    "Your questions should be a mixture of general, high-level concepts, and also highly detailed questions about specific points, including any mathematical or numerical results.\n",
    "You should respond in JSON format according to the following schema:\n",
    "\n",
    "{{ schema }}\n",
    "\n",
    "You should generate {{ number }} question and answer pairs.\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_qa = load_template(\n",
    "    \"prompts/qa_generation_system_prompt.jinja\",\n",
    "    {\n",
    "        \"number\" : 10,\n",
    "        \"schema\" : QAPairs.model_json_schema()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a reading comprehension system that is an expert at extracting information from academic papers.\n",
      "Your task is to carefully read the provided text \"CONTEXT\" and then generate question and answer pairs.\n",
      "Your questions should be concise. Your answers should be as detailed as possible, including any mathematical or numerical results from the text.\n",
      "You should aim to produce approximately one paragraph for your answers (100-200 words).\n",
      "Your questions should be a mixture of general, high-level concepts, and also highly detailed questions about specific points, including any mathematical or numerical results.\n",
      "You should respond in JSON format according to the following schema:\n",
      "\n",
      "{'properties': {'questions': {'items': {'type': 'string'}, 'title': 'List of questions', 'type': 'array'}, 'answers': {'items': {'type': 'string'}, 'title': 'List of answers', 'type': 'array'}}, 'required': ['questions', 'answers'], 'title': 'QAPairs', 'type': 'object'}\n",
      "\n",
      "You should generate 10 question and answer pairs.\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to the pages of the pdf as a single text string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = \" \".join([doc.text for doc in documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are in a position to generate our question answer pairs using `gpt-4o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "user_prompt = (\n",
    "    f\"CONTEXT:\\n\\n{pdf_text}\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt_qa},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the `QAPairs` object using the LLM output, and also save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = QAPairs(**json.loads(response.choices[0].message.content))\n",
    "\n",
    "# save the Q&A to file\n",
    "with open(\"data/qa.json\", \"w\") as f:\n",
    "    json.dump(questions_answers.dict(), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an example look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the main philosophical debates surrounding large language models (LLMs) like GPT-4?\n",
      "---\n",
      "The philosophical debates surrounding LLMs like GPT-4 focus on their linguistic and cognitive competence. These debates echo classic discussions about artificial neural networks as cognitive models. Key topics include compositionality, language acquisition, semantic competence, grounding, world models, and cultural knowledge transmission. The success of LLMs challenges long-held assumptions about neural networks, but further empirical investigation is needed to understand their internal mechanisms. Philosophers are particularly interested in whether LLMs can model human cognitive processes better than classical symbolic models, and whether they possess genuine understanding or merely mimic human-like responses.\n"
     ]
    }
   ],
   "source": [
    "print(questions_answers.questions[0])\n",
    "print('---')\n",
    "print(questions_answers.answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try and do cosine similarity scores between the returned contexts and the actual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main philosophical debates surrounding large language models (LLMs) like GPT-4 include:\n",
      "\n",
      "1. **Cognitive Competence**: There is ongoing disagreement about whether LLMs can be meaningfully ascribed linguistic or cognitive competence, echoing classic debates about the status of artificial neural networks as cognitive models.\n",
      "\n",
      "2. **Intelligence vs. Behavior**: Philosophers question the link between intelligence and observable behavior, as LLMs can produce human-like responses without necessarily understanding the inputs, leading to discussions about whether they are merely \"Blockheads\" that regurgitate information.\n",
      "\n",
      "3. **Data Contamination**: Concerns about \"data contamination\" arise when LLMs' training sets include the very questions they are assessed on, complicating comparisons between human and LLM performance.\n",
      "\n",
      "4. **Internal Mechanisms**: There is a need for further empirical investigation to understand the internal mechanisms of LLMs, as their ability to generate novel outputs raises questions about the nature of intelligence and creativity.\n",
      "\n",
      "5. **Philosophical Implications**: The success of LLMs challenges long-held assumptions in cognitive science and philosophy of language, prompting new philosophical questions about artificial intelligence and its implications for understanding human cognition.\n"
     ]
    }
   ],
   "source": [
    "from utils import rag_query\n",
    "\n",
    "example_query = questions_answers.questions[0]\n",
    "\n",
    "response, context = rag_query(\n",
    "    query=example_query,\n",
    "    n_context=5,\n",
    "    doc_db=doc_db,\n",
    "    return_context=True\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity\n",
    "First look at semantic similarity between the predicted response and the desired response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response_embedding = client.embeddings.create(\n",
    "    input=response,\n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding\n",
    "\n",
    "answer_embedding = client.embeddings.create(\n",
    "    input=questions_answers.answers[0],\n",
    "    model=\"text-embedding-3-small\"\n",
    ").data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85309252]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([response_embedding], [answer_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main philosophical debates surrounding large language models (LLMs) like GPT-4 include:\n",
      "\n",
      "1. **Cognitive Competence**: There is ongoing disagreement about whether LLMs can be meaningfully ascribed linguistic or cognitive competence, echoing classic debates about the status of artificial neural networks as cognitive models.\n",
      "\n",
      "2. **Intelligence vs. Behavior**: Philosophers question the link between intelligence and observable behavior, as LLMs can produce human-like responses without necessarily understanding the inputs, leading to discussions about whether they are merely \"Blockheads\" that regurgitate information.\n",
      "\n",
      "3. **Data Contamination**: Concerns about \"data contamination\" arise when LLMs' training sets include the very questions they are assessed on, complicating comparisons between human and LLM performance.\n",
      "\n",
      "4. **Internal Mechanisms**: There is a need for further empirical investigation to understand the internal mechanisms of LLMs, as their ability to generate novel outputs raises questions about the nature of intelligence and creativity.\n",
      "\n",
      "5. **Philosophical Implications**: The success of LLMs challenges long-held assumptions in cognitive science and philosophy of language, prompting new philosophical questions about artificial intelligence and its implications for understanding human cognition.\n",
      "---\n",
      "The philosophical debates surrounding LLMs like GPT-4 focus on their linguistic and cognitive competence. These debates echo classic discussions about artificial neural networks as cognitive models. Key topics include compositionality, language acquisition, semantic competence, grounding, world models, and cultural knowledge transmission. The success of LLMs challenges long-held assumptions about neural networks, but further empirical investigation is needed to understand their internal mechanisms. Philosophers are particularly interested in whether LLMs can model human cognitive processes better than classical symbolic models, and whether they possess genuine understanding or merely mimic human-like responses.\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "print('---')\n",
    "print(questions_answers.answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "This is a little more complicated. First, we get an LLM to extract key statements from the answer. For example:\n",
    "\n",
    "```python\n",
    "[\n",
    "    ['This study was conducted by Mallinson et al.'],\n",
    "    ['The main focus is to investigate avalanches and criticality in self-organized nanoscale network.']\n",
    "    ['They analyzed electrical conductance.']\n",
    "    ['They analyzed the behavior of the networks under various stimulus conditions.']\n",
    "]\n",
    "```\n",
    "\n",
    "We then ask a second LLM to look at each statement and see if that statement can be inferred from the text, assigning a score of 0 for no, and 1 for yes.\n",
    "\n",
    "To do this, we create two additional Pydantic classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statements(BaseModel):\n",
    "    simpler_statements: list[str] = Field(..., description=\"the simpler statements\")\n",
    "\n",
    "\n",
    "class StatementFaithfulnessAnswer(BaseModel):\n",
    "    statement: str = Field(..., description=\"the original statement, word-for-word\")\n",
    "    reason: str = Field(..., description=\"the reason of the verdict\")\n",
    "    verdict: int = Field(..., description=\"the verdict(0/1) of the faithfulness.\")\n",
    "\n",
    "\n",
    "class Faithfulness(BaseModel):\n",
    "    answers: list[StatementFaithfulnessAnswer] = Field(..., description=\"the faithfulness answers\")\n",
    "    score: float = Field(..., description=\"the faithfulness score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create two more prompts `statement_instruction`, and `faithfulness_instruction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "Given a piece of text, analyze the complexity of each sentence and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON, according to the following schema:\n",
    "\n",
    "{{ schema }}\n",
    "\n",
    "Here is a new piece of text:\n",
    "\n",
    "{{ statement }}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "Your task is to judge the faithfulness of a statement based on a given context. For the statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n",
    "\n",
    "You will give the exact statement, the reason, and the verdict.\n",
    "\n",
    "Format the outputs in JSON, according to the following schema:\n",
    "\n",
    "{{ schema }}\n",
    "\n",
    "Here is a statement:\n",
    "\n",
    "{{ statement }}\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statements(answer):\n",
    "    prompt = load_template(\n",
    "        \"prompts/faithfulness/statement_instruction.jinja\",\n",
    "        {\n",
    "            \"schema\" : Statements.model_json_schema(),\n",
    "            \"text\" : answer\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": answer}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return Statements(**json.loads(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = get_statements(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main philosophical debates surrounding large language models (LLMs) like GPT-4 include:\n",
      "\n",
      "1. **Cognitive Competence**: There is ongoing disagreement about whether LLMs can be meaningfully ascribed linguistic or cognitive competence, echoing classic debates about the status of artificial neural networks as cognitive models.\n",
      "\n",
      "2. **Intelligence vs. Behavior**: Philosophers question the link between intelligence and observable behavior, as LLMs can produce human-like responses without necessarily understanding the inputs, leading to discussions about whether they are merely \"Blockheads\" that regurgitate information.\n",
      "\n",
      "3. **Data Contamination**: Concerns about \"data contamination\" arise when LLMs' training sets include the very questions they are assessed on, complicating comparisons between human and LLM performance.\n",
      "\n",
      "4. **Internal Mechanisms**: There is a need for further empirical investigation to understand the internal mechanisms of LLMs, as their ability to generate novel outputs raises questions about the nature of intelligence and creativity.\n",
      "\n",
      "5. **Philosophical Implications**: The success of LLMs challenges long-held assumptions in cognitive science and philosophy of language, prompting new philosophical questions about artificial intelligence and its implications for understanding human cognition.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Statements</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">simpler_statements</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ongoing disagreement exists about whether large language models can be meaningfully ascribed linguistic or cognitive competence.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'Classic debates about the status of artificial neural networks as cognitive models echo this disagreement.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'Philosophers question the link between intelligence and observable behavior.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large language models can produce human-like responses without necessarily understanding the inputs.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Discussions arise about whether large language models are merely 'Blockheads' that regurgitate information.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'Concerns about data contamination arise when training sets of large language models include the very questions that are assessed.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'This situation complicates comparisons between human performance and large language model performance.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'Further empirical investigation is needed to understand the internal mechanisms of large language models.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'The ability of large language models to generate novel outputs raises questions about the nature of intelligence and creativity.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'The success of large language models challenges long-held assumptions in cognitive science and philosophy of language.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'New philosophical questions about artificial intelligence arise from this success.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'These questions have implications for understanding human cognition.'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mStatements\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33msimpler_statements\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'Ongoing disagreement exists about whether large language models can be meaningfully ascribed linguistic or cognitive competence.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'Classic debates about the status of artificial neural networks as cognitive models echo this disagreement.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'Philosophers question the link between intelligence and observable behavior.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'Large language models can produce human-like responses without necessarily understanding the inputs.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m\"Discussions arise about whether large language models are merely 'Blockheads' that regurgitate information.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'Concerns about data contamination arise when training sets of large language models include the very questions that are assessed.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'This situation complicates comparisons between human performance and large language model performance.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'Further empirical investigation is needed to understand the internal mechanisms of large language models.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'The ability of large language models to generate novel outputs raises questions about the nature of intelligence and creativity.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'The success of large language models challenges long-held assumptions in cognitive science and philosophy of language.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'New philosophical questions about artificial intelligence arise from this success.'\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[32m'These questions have implications for understanding human cognition.'\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.pretty import pprint\n",
    "print(response)\n",
    "pprint(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithfulness(statements : Statements, context):\n",
    "    context_joined = \" \".join(context)\n",
    "    faithfulness_answers = []\n",
    "\n",
    "    for statement in statements.simpler_statements:\n",
    "        prompt = load_template(\n",
    "            \"prompts/faithfulness/faithfulness_instruction.jinja\",\n",
    "            {\n",
    "                \"schema\" : StatementFaithfulnessAnswer.model_json_schema(),\n",
    "                \"statement\" : statement,\n",
    "                \"context\" : context_joined\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": context_joined}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        faithfulness_answers.append(StatementFaithfulnessAnswer(**json.loads(response)))\n",
    "\n",
    "    score = sum([answer.verdict for answer in faithfulness_answers]) / len(faithfulness_answers)\n",
    "\n",
    "    return Faithfulness(answers=faithfulness_answers, score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_faithfulness(statements, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Faithfulness</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">answers</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Ongoing disagreement exists about whether large language models can be meaningfully ascribed linguistic or cognitive competence.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context explicitly states that there are ongoing disagreements regarding the extent to which we can meaningfully ascribe linguistic or cognitive competence to language models, which directly supports the statement.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Classic debates about the status of artificial neural networks as cognitive models echo this disagreement.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context discusses ongoing disagreements about the extent to which linguistic or cognitive competence can be ascribed to language models, and it explicitly mentions that these questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. Therefore, the statement can be directly inferred from the context.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Philosophers question the link between intelligence and observable behavior.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context discusses how philosophers have debated the relationship between intelligence and behavior, particularly in the context of artificial intelligence and language models. It mentions that the consensus among philosophers is that intelligence does not merely depend on observable behaviors but also on internal processing mechanisms. This indicates that philosophers indeed question the link between intelligence and observable behavior.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Large language models can produce human-like responses without necessarily understanding the inputs.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The context discusses how large language models, like GPT-4, can generate responses that may appear human-like but do not necessarily involve understanding the inputs in a meaningful way. It compares this to a hypothetical system called 'Blockhead' that produces responses without understanding, indicating that LLMs can also operate in a similar manner.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Discussions arise about whether large language models are merely 'Blockheads' that regurgitate information.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The context discusses the philosophical debate surrounding large language models (LLMs) like GPT-4, including the notion that they might be seen as 'Blockheads' that simply regurgitate information without true understanding. This indicates that discussions about this perspective are indeed present in the context.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Concerns about data contamination arise when training sets of large language models include the very questions that are assessed.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The context explicitly states that 'data contamination' is a serious concern when the training set contains the very questions on which the LLM's abilities are assessed, indicating that such concerns do indeed arise in this scenario.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'This situation complicates comparisons between human performance and large language model performance.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The context discusses the challenges in comparing human performance and large language model performance, particularly highlighting concerns such as 'data contamination' and the differences in the underlying mechanisms of human intelligence versus LLMs. This implies that the situation indeed complicates such comparisons.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Further empirical investigation is needed to understand the internal mechanisms of large language models.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context explicitly states that there is a need for further empirical investigation to better understand the internal mechanisms of language models, which directly supports the statement.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The ability of large language models to generate novel outputs raises questions about the nature of intelligence and creativity.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context discusses how large language models, such as GPT-4, can produce genuinely novel outputs by flexibly blending patterns from their training data. This capability prompts philosophical questions regarding the nature of intelligence and creativity, as it challenges traditional views on cognitive competence and the definition of intelligence.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The success of large language models challenges long-held assumptions in cognitive science and philosophy of language.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context explicitly states that the authors argue that the success of language models challenges several long-held assumptions about artificial neural networks, which relates directly to cognitive science and philosophy of language.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'New philosophical questions about artificial intelligence arise from this success.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context discusses how the success of language models like GPT-4 prompts new philosophical questions and challenges long-held assumptions about artificial neural networks. It explicitly states that the developments in language models lead to new philosophical inquiries, which directly supports the statement.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StatementFaithfulnessAnswer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">statement</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'These questions have implications for understanding human cognition.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The context discusses the implications of language models like GPT-4 on understanding cognitive competence and intelligence, which relates to human cognition. However, it does not explicitly state that the questions posed have direct implications for understanding human cognition, making the statement not directly inferable.'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">verdict</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9166666666666666</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mFaithfulness\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33manswers\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'Ongoing disagreement exists about whether large language models can be meaningfully ascribed linguistic or cognitive competence.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context explicitly states that there are ongoing disagreements regarding the extent to which we can meaningfully ascribe linguistic or cognitive competence to language models, which directly supports the statement.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'Classic debates about the status of artificial neural networks as cognitive models echo this disagreement.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context discusses ongoing disagreements about the extent to which linguistic or cognitive competence can be ascribed to language models, and it explicitly mentions that these questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. Therefore, the statement can be directly inferred from the context.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'Philosophers question the link between intelligence and observable behavior.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context discusses how philosophers have debated the relationship between intelligence and behavior, particularly in the context of artificial intelligence and language models. It mentions that the consensus among philosophers is that intelligence does not merely depend on observable behaviors but also on internal processing mechanisms. This indicates that philosophers indeed question the link between intelligence and observable behavior.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'Large language models can produce human-like responses without necessarily understanding the inputs.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m context discusses how large language models, like GPT-4, can generate responses that may appear human-like but do not necessarily involve understanding the inputs in a meaningful way. It compares this to a hypothetical system called 'Blockhead' that produces responses without understanding, indicating that LLMs can also operate in a similar manner.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m\"Discussions\u001b[0m\u001b[32m arise about whether large language models are merely 'Blockheads' that regurgitate information.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m context discusses the philosophical debate surrounding large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m like GPT-4, including the notion that they might be seen as 'Blockheads' that simply regurgitate information without true understanding. This indicates that discussions about this perspective are indeed present in the context.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'Concerns about data contamination arise when training sets of large language models include the very questions that are assessed.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m context explicitly states that 'data contamination' is a serious concern when the training set contains the very questions on which the LLM's abilities are assessed, indicating that such concerns do indeed arise in this scenario.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'This situation complicates comparisons between human performance and large language model performance.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m context discusses the challenges in comparing human performance and large language model performance, particularly highlighting concerns such as 'data contamination' and the differences in the underlying mechanisms of human intelligence versus LLMs. This implies that the situation indeed complicates such comparisons.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'Further empirical investigation is needed to understand the internal mechanisms of large language models.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context explicitly states that there is a need for further empirical investigation to better understand the internal mechanisms of language models, which directly supports the statement.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'The ability of large language models to generate novel outputs raises questions about the nature of intelligence and creativity.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context discusses how large language models, such as GPT-4, can produce genuinely novel outputs by flexibly blending patterns from their training data. This capability prompts philosophical questions regarding the nature of intelligence and creativity, as it challenges traditional views on cognitive competence and the definition of intelligence.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'The success of large language models challenges long-held assumptions in cognitive science and philosophy of language.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context explicitly states that the authors argue that the success of language models challenges several long-held assumptions about artificial neural networks, which relates directly to cognitive science and philosophy of language.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'New philosophical questions about artificial intelligence arise from this success.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context discusses how the success of language models like GPT-4 prompts new philosophical questions and challenges long-held assumptions about artificial neural networks. It explicitly states that the developments in language models lead to new philosophical inquiries, which directly supports the statement.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m1\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStatementFaithfulnessAnswer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstatement\u001b[0m=\u001b[32m'These questions have implications for understanding human cognition.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mreason\u001b[0m=\u001b[32m'The context discusses the implications of language models like GPT-4 on understanding cognitive competence and intelligence, which relates to human cognition. However, it does not explicitly state that the questions posed have direct implications for understanding human cognition, making the statement not directly inferable.'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mverdict\u001b[0m=\u001b[1;36m0\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mscore\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.9166666666666666\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
