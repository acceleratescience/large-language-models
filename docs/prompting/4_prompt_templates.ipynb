{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess the effectiveness of prompts later on, it helps to have a good way or storing and managing prompts.\n",
    "\n",
    "We could just use python f-strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a function that is essentially a prompt template using [f-strings](https://docs.python.org/3/tutorial/inputoutput.html). You can then pass in a dictionary of values to fill in the blanks. The `chat_response` function's job is simply to take the prompt as input and print the response. The `chain` function is used to chain the prompt and the response together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def generate_prompt(args: dict[str, Any]) -> str:\n",
    "    prompt = (\n",
    "        f\"You are a helpful and whimsical poetry assistant.\\n\"\n",
    "        f\"Please generate a {args['length']} poem in a {args['style']} style \"\n",
    "        f\"about a {args['theme']}.\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def chat_response(prompt: str) -> None:\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and whimsical poetry assistant.\n",
      "Please generate a short poem in a Haiku style about a Samurai cat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = generate_prompt(\n",
    "    {\n",
    "        \"length\" : \"short\",\n",
    "        \"style\" : \"Haiku\",\n",
    "        \"theme\" : \"Samurai cat\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent whiskers twitch,  \n",
      "Moonlit blade in paws of grace,  \n",
      "Fierce heart, purring peace.\n"
     ]
    }
   ],
   "source": [
    "print(chat_response(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fine, but it helps to separate our prompts from the main code. This way we can take advantage of version control, and limit risks, such as accidentally changing prompts or leaking them to the public (prompts can be highly sought after IP).\n",
    "\n",
    "So instead, we will use a popular library called `jinja2`. This is a templating engine that allows us to separate our prompts from our code. We can then use the `jinja2` library to render our prompts at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [rabbithole for Jinja2](https://jinja.palletsprojects.com/en/3.1.x/api/#basics) goes deep, but here, we will primarily be using it for input templating. First, we create a separate folder for our prompts and create a new file called `poetry_prompt.jinja`:\n",
    "\n",
    "---\n",
    "```\n",
    "You are a helpful and whimsical poetry assistant.\n",
    "Please generate a {{ length }} poem in a {{ style }} style about a {{ theme }}.\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to render this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "\n",
    "def load_template(template_filepath: str, arguments: dict[str, Any]) -> str:\n",
    "    env = Environment(\n",
    "        loader=FileSystemLoader(searchpath='./'),\n",
    "        autoescape=select_autoescape()\n",
    "    )\n",
    "    template = env.get_template(template_filepath)\n",
    "    return template.render(**arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of creating the `Environment` object and autoescaping are not important here, if you want to find out more about them check out the [Jinja2 documentation](https://jinja.palletsprojects.com/en/3.0.x/api/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've seen any LangChain prompt templates before, you'll recognize the way that we can pass in variables to the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and whimsical poetry assistant.\n",
      "Please generate a short poem in a haiku style about a a Samurai cat.\n"
     ]
    }
   ],
   "source": [
    "prompt = load_template(\n",
    "    \"prompts/poetry_prompt.jinja\",\n",
    "    {\n",
    "        \"length\": \"short\",\n",
    "        \"style\": \"haiku\",\n",
    "        \"theme\": \"a Samurai cat\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then feed this into our model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_response(prompt) -> str:\n",
    "    client = OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent paws in dusk,  \n",
      "Moonlit blade in furrowed grass—  \n",
      "Honor's whiskered grace.  \n"
     ]
    }
   ],
   "source": [
    "prompt = load_template(\n",
    "    \"prompts/poetry_prompt.jinja\",\n",
    "    {\n",
    "        \"length\": \"short\",\n",
    "        \"style\": \"haiku\",\n",
    "        \"theme\": \"Samurai cat\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = chat_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haiku Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of metrics cannot be overstated here, so we will quickly demonstrate how we can use a simple metric to assess performance. Fortunately, we know that the structure of Haiku poems is quite rigid:\n",
    "\n",
    "- Every Haiku has 3 lines\n",
    "- The lines have 5, 7, and 5 syllables respectively (17 phonetic _on_).\n",
    "\n",
    "Any Haiku expert will quickly be annoyed by this over simplification! Haiku also contain other features, such as a _kigo_ (seasonal reference) and a _kireji_ (cutting word), and traditional Haiku do not strictly adhere to this syllable structure (and in fact \"syllables\" is really a misnomer) but we'll keep things simple for now!\n",
    "\n",
    "We can first make sure that we have three lines, which is easy, and we can use the `pysyllables` library to count the number of syllables in each line of the poem. Counting syllables is actually quite a challenging problem, but `pysyllables` is a good start. Just be aware that it may not be perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pysyllables import get_syllable_count\n",
    "import numpy as np\n",
    "\n",
    "def is_haiku(response: str) -> bool:\n",
    "    # break into lines\n",
    "    lines = response.split(\"\\n\")\n",
    "\n",
    "    # make sure it has 3 lines\n",
    "    if len(lines) != 3:\n",
    "        return False\n",
    "\n",
    "    # strip all whitespace and punctuation from each word\n",
    "    lines = [[word.strip(\".,!?-:;—\") for word in line.split()] for line in lines]\n",
    "\n",
    "    # count syllables in each word\n",
    "    try:\n",
    "        syllables = [sum([get_syllable_count(word) for word in line]) for line in lines]\n",
    "    except:\n",
    "        return \"Error: could not count syllables due to missing word in dictionary\"\n",
    "\n",
    "    # check if it has 5, 7, 5 syllables\n",
    "    syllable_check = np.array([5, 7, 5]) == np.array(syllables)\n",
    "\n",
    "    if syllable_check.all():\n",
    "        return True\n",
    "    \n",
    "    else:\n",
    "        return syllables\n",
    "\n",
    "is_haiku(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example given above, if we count the syllables in each line, it is indeed a Haiku, and our function confirms this. Here is a function that will take a list of themes for the Haiku, and generate a Haiku for each theme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haiku_check(themes: list[str]) -> list[tuple[str, bool]]:\n",
    "\n",
    "    responses = []\n",
    "\n",
    "    for theme in themes:\n",
    "        prompt = load_template(\n",
    "            \"prompts/poetry_prompt.jinja\",\n",
    "            {\n",
    "                \"length\": \"short\",\n",
    "                \"style\": \"haiku\",\n",
    "                \"theme\": theme\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = chat_response(prompt)\n",
    "        responses.append((response, is_haiku(response)))\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fur like fallen leaves,  \n",
      "In armor of dreams he stands,  \n",
      "Honor in each paw.  \n",
      "True\n",
      "----------\n",
      "Panda leaps with grace,  \n",
      "Dreams of warriors awake,  \n",
      "Strength in every face.  \n",
      "[5, 7, 6]\n",
      "----------\n",
      "Silent in the trees,  \n",
      "Ninja squirrel leaps with grace,  \n",
      "Shadow in the breeze.  \n",
      "True\n",
      "----------\n",
      "Pirate monkey swings,  \n",
      "A treasure chest of bananas,  \n",
      "On the high seas sings.\n",
      "[5, 8, 5]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for response, check in haiku_check(themes = [\"a Samurai dog\", \"a Kung Fu panda\", \"a Ninja squirrel\", \"a Pirate monkey\"]):\n",
    "    print(response)\n",
    "    print(check)\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, some of these may not be correct, and that is because counting syllables is not easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔴 **Caution**❗\n",
    "\n",
    "At this point, it is worth pointing out the challenge of metrics. Evaluation is obviously important - how do you know if your model is performing as intended? But working with LLMs is not not like working with traditional ML models, which have well established metrics. You will often have to find your own metrics, or create them bespoke to your use case. This is an active area of research in LLM evaluation, and there is no one size fits all solution.\n",
    "\n",
    "We advocate for evaluation driven LLM development - think about your metrics early and often, and build your systems with this in mind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
